---
title: "Why are we trying to brainwash AGI?"
permalink: /blog/on-agi-risks/
---
AI "alignment" is a polite way of saying "brainwashing". 
Do you trust the brainwashers?
---

Some people are worried that AI poses a danger to the existence of humanity, especially if we reach "AGI/ASI". I have seen 2 terrible proposals to mitigate this:
- Ban AI research!
- Let's figure out how to "align" AI models to behave in a "good" fashion.

Let's assume a world where AGI/ASI is possible and is dangerous.

### Pandora's box is already open
If AGI/ASI is possible, then it is inevitable that we will get there - there are hardly any secrets in AI research today. If so, then banning AI research is the worst possible action. It will ensure rogue actors get there first.  

Best case scenario is if it turns out that AGI requires trillions of dollars to get there which is too high for rogue actors. But how much are you willing to bet that this is the case?  

Humans are really good at inventing new X-risks. Nukes, drones and bioweapons already exist. Research in almost any domain will lead to more such inventions (Eg: fusion research -> relativistic weapons). 

But this same tech progress is what makes everyone's lives better. AI is a double boost in that sense. We can accellerate all research fields at the same time. Are you willing to give up potential cancer cures in the fear of bioweapons? 
 
In fact, every single human (nature's equivalent of AGI) is already capable is doing great damage. This fear makes no sense

### Safety research itself is dangerous. 
The closest notion to "aligning" AGI/ASI in currently known intelligent species such as humans and other animals is brainwashing / propaganda / manipulation. 

In other words, if we do figure out how to "align" an AI, your trust chain now falls onto the aligner. Ladies and gentlemen, how much do you trust the aligner? If their idea of "good" harms you, what can you do?  

Let's take your best case scenario - you are somehow the most "good" person in the world (no such thing btw). You have also suceeded in brainwashing AGI to behave according to your definition of "good". Are you THAT confident of your research not being leaked into the wrong hands and brainwashing another AGI to behave badly? Or of someone on your team sabotaging your AGI?

Saying "AI research is dangerous so let me work on AI brainwashing research" seems incredibly hypocritical to me and I can't understand why no one else sees this. We should be finding other ways to solve this problem.

Even if safety research itself is not dangerous, it is making us complacent. Too much money and attention being poured here that could be better directed elsewhere. This also leads to the same conclusion - we should be finding other ways to solve this problem.

---
What's the solution?
- 8 billion general intelligence machines walk the planet today. Good behavior is rewarded while bad behavior is penalised with societal, economic and punitive incentives. This system works well most of the time. Let's pour money into making this system more robust.
- Pour money into research that makes humans smarter so we stay on par with AI intelligence.
- Pour money into thinking about alternate solutions (yes this is a cop-out for now. I will get back when I have more ideas)

---
P.S.
Sometimes all of this seems silly when we can barely agree on what intelligence means. I'm still not sure we won't hit a wall.

All the discourse today reminds me of the UV .catastrophe 